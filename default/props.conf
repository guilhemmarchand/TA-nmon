# props.conf

##################################
#			nmon2csv stanza			#
##################################

# IMPORTANT: Since the V1.6.0, the configuration uses the nmon2csv.sh wrapper that will call nmon2csv.py (prefered choice) or nmon2csv.pl if appropriated
# You can still force the configuration to use the Python or Perl converter (but the nmon2csv.sh wrapper will do the choice for you, enforcing the choice is not required anymore)

# *** BE UPGRADE RESILIENT: *** Copy this stanza to your local/props.conf to prevent futur upgrades from overwritting your setting 

# To force the use of the Python converter, set:
# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.py

# To force the use of the Python converter, set:
# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.pl

# This source stanza will be called by the archive processor to convert NMON raw data into csv files
# Splunk can manage. See inputs.conf for the associated monitor

# To force nmon2csv parsers to always consider nmon files as realtime data, you can set the option "--mode realtime":
# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh --mode realtime
# This is normally not necessary as parsers will automatically detect if we are dealing with realtime data

# host name definition: by default, nmon2csv parsers will use the value returned by nmon for the host name definition. (available in the config section - AAA,host)
# If you want to have hosts using their fully qualified domain name (fqdn) instead of the nmon hostname value, add the option "--use_fqdn" in the source stanza definition
# the value of host name will be equivalent to the "hostname -f" command on the host.
# **CAUTION**: Do use this option when managing nmon data generated out of Splunk (central repositories) or all the data being ingested will be identified as coming the host
# that managed the nmon data
# This option must be used on host that are only managing their own nmon data.

# To activate fqdn:
# unarchive_cmd = $SPLUNK_HOME/bin/splunk cmd $SPLUNK_HOME/etc/apps/nmon/bin/nmon2csv.sh --use_fqdn

### CSV indexing versus JSON indexing ###

# New since version 1.2.55 of the addons, you can choose to generate the performance data into json data instead of legacy csv data

# This can be activated in a custom local/props.conf with the addition of the "--json_output" argument, example:

# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh --mode realtime --json_output

# In inputs.conf, you can as well choose between 2 ways of indexing the json data:

# - "sourcetype = nmon_data:json:extracted" and "source = perfdata:json:extracted": Index the json data as raw data and extract the fields at search time, this saves storage at the price of lower performances
# - "sourcetype = nmon_data:json:indexed" and "source = perfdata:json:indexed": Index the json data as json indexed, best performances at the cost of more storage requirements

# The default of inputs.conf used the search time extracted mode, saves storage but provides lower performance in raw SPL searches

# See the following documentation for more information about implications in term of performance, volume of data and licensing costs, as well as long term storage requirements

# http://nmon-for-splunk.readthedocs.io/en/latest/csv_indexing_vs_json_indexing.html

# Finally, the json mode is available currently only for the Python parser, this will have no effects on a server using Perl parser

[source::.../*.nmon]
invalid_cause = archive
unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh --mode realtime
sourcetype = nmon_processing
NO_BINARY_CHECK = true

# To manage repositories archives of cold nmon files (add you own for other compressed formats)
[source::.../*.nmon.gz]
invalid_cause = archive
unarchive_cmd = gunzip | $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh
sourcetype = nmon_processing
NO_BINARY_CHECK = true

###########################################
#			nmon converted csv stanza			#
###########################################

# This sourcetype stanza will be used to index nmon csv converted data
# Every generated csv file will contain a CSV header used by Splunk to identify fields

[nmon_data]

FIELD_DELIMITER=,
FIELD_QUOTE="
HEADER_FIELD_LINE_NUMBER=1

# your settings
INDEXED_EXTRACTIONS=csv
NO_BINARY_CHECK=1
SHOULD_LINEMERGE=false
TIMESTAMP_FIELDS=ZZZZ
TIME_FORMAT=%d-%m-%Y %H:%M:%S

# set by detected source type
KV_MODE=none
pulldown_type=true

# Leaving PUNCT enabled can impact indexing performance, and uses space
# For structured data, it has poor interest and shall be deactivated
ANNOTATE_PUNCT=false

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_data_hostoverride

#
# search time extractions: The following stanzas will occur at search time only
#

########################################
# CIM normalization / NMON extractions #
########################################

# When applicable, be CIM compliant

# Various CIM
FIELDALIAS-dest = host as dest
EVAL-hypervisor_id = if(isnotnull(frameID), frameID, serialnum)

#
# Non CIM and Power arch common, unify AIX and PowerLinux stats to a common model at search extraction time
#

# For Power arch micro-partitions (AIX & PowerLinux)
EVAL-lpar_vp_usage = case(OStype=="AIX", round(((VP_User_PCT+VP_Sys_PCT+VP_Wait_PCT+VP_Idle_PCT)*virtualCPUs/100),3), OStype=="Linux", round(PhysicalCPU,3))
EVAL-lpar_vp_usage_PCT = case(OStype=="AIX", round((VP_User_PCT+VP_Sys_PCT+VP_Wait_PCT+VP_Idle_PCT),2), OStype=="Linux", round((PhysicalCPU/partition_active_processors*100),2))

# For Power arch micro-partitions Pool statistics (AIX & PowerLinux)
# Note: For PowerLinux, at the perf collection startup the pool usage may report almost 100% usage which is incorrect, filtering with pool_idle_time>"0.1" will prevent from loading these data

EVAL-lpar_pool_vp_usage = case(OStype=="AIX", round((poolCPUs-PoolIdle),3), OStype="Linux", case(pool_idle_time>"0.1" AND pool_idle_time<(pool_capacity/100), round((((pool_capacity/100)-pool_idle_time)/smt_mode),3)) )
EVAL-lpar_pool_vp_usage_PCT = case(OStype=="AIX", round(((poolCPUs-PoolIdle)*100/poolCPUs),2), OStype="Linux", case(pool_idle_time>"0.1" AND pool_idle_time<(pool_capacity/100), round((((pool_capacity/100)-pool_idle_time)/smt_mode)/(pool_capacity/100)*100,2)) )

# Normalize capacity information
EVAL-lpar_pool_capacity = case(OStype=="AIX", poolCPUs, OStype="Linux", (pool_capacity/100))
EVAL-lpar_vp_capacity = case(OStype=="AIX", virtualCPUs, OStype="Linux", partition_active_processors)
EVAL-lpar_entitled_capacity = case(OStype=="AIX", entitled, OStype="Linux", partition_entitled_capacity)

# CIM normalization for cpu metrics

EVAL-cpu_load_percent = case(match(type,"CPU.*"), (Sys_PCT+User_PCT+Wait_PCT), type=="TOP", pct_CPU, type=="LPAR", round((VP_User_PCT+VP_Sys_PCT+VP_Wait_PCT+VP_Idle_PCT),2), OStype=="Linux", round((PhysicalCPU/partition_active_processors*100),2) )
FIELDALIAS-cpu_load_user = User_PCT as cpu_user_percent
# Non CIM, but provides good visibility on these fields, and this looks like logical
FIELDALIAS-cpu_load_sys = Sys_PCT as cpu_sys_percent
FIELDALIAS-cpu_load_wait = Wait_PCT as cpu_wait_percent
FIELDALIAS-cpu_load_idle = Idle_PCT as cpu_idle_percent

# Non CIM: provide the same fields evaluation in logical CPU units
EVAL-cpu_load_logicalcpus = case(match(type,"CPU_ALL"), ((Sys_PCT+User_PCT+Wait_PCT)/100*logical_cpus))
EVAL-cpu_user_logicalcpus = case(match(type,"CPU_ALL"), (User_PCT/100*logical_cpus))
EVAL-cpu_wait_logicalcpus = case(match(type,"CPU_ALL"), (Wait_PCT/100*logical_cpus))
EVAL-cpu_idle_logicalcpus = case(match(type,"CPU_ALL"), (Idle/100*logical_cpus))

# CIM normalization for common memory metrics

# Total memory available in MB (for AIX / Linux / Solaris)
EVAL-mem = case(isnotnull(Real_total_MB), Real_total_MB, isnotnull(memtotal), memtotal)
EVAL-mem_free = case(isnotnull(Real_free_MB), Real_free_MB, isnotnull(memfree), memfree)
EVAL-mem_used = case(type=="MEM", (case(isnotnull(Real_total_MB), (Real_total_MB-Real_free_MB), isnotnull(memtotal), (memtotal-memfree))), type="TOP", case(OStype="AIX", ((ResData+ResText)), OStype="Linux", (ResSet), OStype="Solaris", ResSize))

# Non CIM: For Linux, create the effective mem_used and mem_free, effective means memory less buffers and cached that can be free by the kernel if it required
# Volume of buffers and cached are by default accounted in the memory being used, therefore this is can be considered as a level memory available for system and application needs
# For other OS, these fields will be created for convenient use but are they are equal to previously calculated fields
EVAL-mem_used_effective = case(type=="MEM", (case(isnotnull(Real_total_MB), (Real_total_MB-Real_free_MB), isnotnull(cached), ((memtotal-(buffers+cached))-memfree), isnotnull(memtotal), memtotal-memfree)))
EVAL-mem_free_effective = case(isnotnull(Real_free_MB), Real_free_MB, isnotnull(cached), (memfree+(buffers+cached)), isnotnull(memtotal), memfree)

# Evaluate at the props level the percentile versions of memory statistics
EVAL-mem_used_PCT = case(isnotnull(Real_total_MB), round((((Real_total_MB-Real_free_MB)/Real_total_MB)*100),2), isnotnull(memtotal), round((((memtotal-memfree)/memtotal)*100),2))
EVAL-mem_free_PCT = case(isnotnull(Real_Free_PCT), Real_Free_PCT, isnotnull(memtotal), round(((memfree/memtotal)*100),2))
EVAL-mem_used_effective_PCT = case(isnotnull(Real_total_MB), round((((Real_total_MB-Real_free_MB)/Real_total_MB)*100),2), isnotnull(cached), round((((((memtotal-(buffers+cached))-memfree))/memtotal)*100),2), isnotnull(memtotal), round((((memtotal-memfree)/memtotal)*100),2))
EVAL-mem_free_effective_PCT = case(isnotnull(Real_Free_PCT), Real_Free_PCT, isnotnull(cached), round((((memfree+(buffers+cached))/memtotal)*100),2), isnotnull(memtotal), round(((memfree/memtotal)*100),2))

# Non CIM: For Linux, eval cached, buffers, active and inactive in PCT
EVAL-cached_PCT = case(isnotnull(cached), round(((cached/memtotal)*100),2))
EVAL-buffers_PCT = case(isnotnull(buffers), round(((buffers/memtotal)*100),2))
EVAL-active_PCT = case(isnotnull(active), round(((active/memtotal)*100),2))
EVAL-inactive_PCT = case(isnotnull(inactive), round(((inactive/memtotal)*100),2))

# Total virtual memory available in MB (for AIX / Linux / Solaris)
EVAL-swap = case(isnotnull(Virtual_total_MB), Virtual_total_MB, isnotnull(swaptotal), swaptotal)
EVAL-swap_free = case(isnotnull(Virtual_free_MB), Virtual_free_MB, isnotnull(swapfree), swapfree)
EVAL-swap_used = case(isnotnull(Virtual_total_MB), (Virtual_total_MB-Virtual_free_MB), isnotnull(swaptotal), (swaptotal-swapfree))

# Non CIM
EVAL-swap_free_PCT = case(isnotnull(Virtual_free_PCT), Virtual_free_PCT, isnotnull(swapfree), round(((swapfree/swaptotal)*100),2))
EVAL-swap_used_PCT = case(isnotnull(Virtual_total_MB), round((((Virtual_total_MB-Virtual_free_MB)/Virtual_total_MB)*100),2), isnotnull(swaptotal), round((((swaptotal-swapfree)/swaptotal)*100),2))

# For Linux, consider effective swap using over the volume of swapcached
EVAL-swap_free_effective = case(isnotnull(Virtual_free_MB), Virtual_free_MB, isnotnull(swapcached), (swapfree+swapcached), isnotnull(swapfree), swapfree)
EVAL-swap_used_effective = case(isnotnull(Virtual_total_MB), (Virtual_total_MB-Virtual_free_MB), isnotnull(swapcached), ((swaptotal-swapfree)-swapcached), isnotnull(swaptotal), (swaptotal-swapfree))
EVAL-swap_free_effective_PCT = case(isnotnull(Virtual_free_PCT), Virtual_free_PCT, isnotnull(swapcached), round((((swapfree+swapcached)/swaptotal)*100),2), isnotnull(swapfree), round(((swapfree/swaptotal)*100),2))
EVAL-swap_used_effective_PCT = case(isnotnull(Virtual_total_MB), round((((Virtual_total_MB-Virtual_free_MB)/Virtual_total_MB)*100),2), isnotnull(swapcached), round((((((swaptotal-swapfree)-swapcached))/swaptotal)*100),2), isnotnull(swaptotal), round((((swaptotal-swapfree)/swaptotal)*100),2))

# Non CIM
EVAL-swapcached_PCT = case(isnotnull(swapcached), round(((swapcached/swaptotal)*100),2))

# CIM normalization for common network statistics
EVAL-thruput = case(type=="NET", (value*1000))
EXTRACT-dvc_name = (?<dvc_name>\w*)\-(?<direction_pattern>read|write)\w*
FIELDALIAS-dvc = dvc_name as dvc
EVAL-direction = case(direction_pattern=="read", "inbound", direction_pattern=="write", "outbound")
EVAL-bytes_in = case(type=="NET", case(direction_pattern=="read", (value*1000)) )
EVAL-bytes_out = case(type=="NET", case(direction_pattern=="write", (value*1000)) )
EVAL-bytes = case(case(type=="NET", direction_pattern=="read"), (value*1000), case(type=="NET", direction_pattern=="write"), (value*1000) )
EVAL-packets_in = case(type=="NETPACKET", case(direction_pattern=="read", (value*1000)) )
EVAL-packets_out = case(type=="NETPACKET", case(direction_pattern=="write", (value*1000)) )
EVAL-packets = case(case(type=="NETPACKET", direction_pattern=="read"), (value*1000), case(type=="NETPACKET", direction_pattern=="write"), (value*1000) )

# CIM for ApplicationState, linked to TOP monitors
FIELDALIAS-Command = Command as process
FIELDALIAS-process_id = PID as process_id
EVAL-user = case(type=="TOP", if(isnotnull(username), username, "undefined"))

# CIM normalization for storage metrics
FIELDALIAS-mount = device as mount
EVAL-storage_used_percent = case(type=="JFSFILE", value)
EVAL-storage_free_percent = case(type=="JFSFILE", (100 - value))

# Non CIM for disk extended statistics (DG*)
EVAL-disk_backlog_time_ms = case(type=="DGBACKLOG", value)
EVAL-disk_busy_time_pct = case(type=="DGBUSY", value)
EVAL-disk_in_flight_io = case(type=="DGINFLIGHT", value)
EVAL-disk_time_spent_for_io_ms = case(type=="DGIOTIME", value)
EVAL-disk_read_KB_per_sec = case(type=="DGREAD", value)
EVAL-disk_merge_read_sec = case(type=="DGREADMERGE", value)
EVAL-disk_read_sec = case(type=="DGREADS", value)
EVAL-disk_read_service_time_ms = case(type=="DGREADSERV", value)
EVAL-disk_block_size_KB = case(type=="DGSIZE", value)
EVAL-disk_write_KB_per_sec = case(type=="DGWRITE", value)
EVAL-disk_merge_write_sec = case(type=="DGWRITEMERGE", value)
EVAL-disk_write_sec = case(type=="DGWRITES", value)
EVAL-disk_write_service_time_ms = case(type=="DGWRITESERV", value)
EVAL-disk_total_iops = case(type=="DGXFER", value)

#
# NMON EXTERNAL
#

# uptime from nmon_external

# extract the load average
EXTRACT-uptime_loadaverage = load_average:\s*(?<load_average_1min>[\d|\.]*);\s*(?<load_average_5min>[\d|\.]*);\s*(?<load_average_15min>[\d|\.]*) in uptime_stdout

# uptime (available for Linux hosts only)
# Sic !!!
EXTRACT-uptime_days = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_days>\d*)[\s|_]*day[s|,|;]*[\s|_]*(?<uptime_dayshour>\d*):(?<uptime_daystime>\d*) in uptime_stdout
EXTRACT-uptime_days_with_min = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_days>\d*)[\s|_]*day[s|,|;]*[\s|_]*(?<uptime_daysmin>\d*)[\s|_]min in uptime_stdout
EXTRACT-uptime_minutes = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_minutes>\d*)[\s|_]*min[\s|_]*[,|;] in uptime_stdout
EXTRACT-uptime_hours = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_hours>\d*:\d*)[\s|_]*[,|;] in uptime_stdout
EXTRACT-uptime_hours_hours = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_hours_hours>\d*):\d*[\s|_]*[,|;] in uptime_stdout
EXTRACT-uptime_hours_minutes = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*\d*:(?<uptime_hours_minutes>\d*)[\s|_]*[,|;] in uptime_stdout
EVAL-uptime_fromdays = (uptime_days*86400)
EVAL-uptime_fromminutes = (uptime_minutes*60)
EVAL-uptime_fromhours = (uptime_hours_hours*60*60) + (uptime_hours_minutes*60)
EVAL-uptime = case(isnotnull(uptime_daysmin), ((uptime_days*86400)+(uptime_daysmin*60)), isnotnull(uptime_days), ((uptime_days*86400)+(uptime_dayshour*3600)+(uptime_daystime*60)), isnotnull(uptime_minutes), (uptime_minutes*60), isnotnull(uptime_hours), (uptime_hours_hours*60*60) + (uptime_hours_minutes*60) )

### ITSI normalization ###

# ITSI has its own data model

# from type-PROC, the number of threads waiting for available processors
FIELDALIAS-wait_threads_count = Runnable as wait_threads_count

# for memory analysis
EVAL-mem_free_percent = case(isnotnull(Real_Free_PCT), Real_Free_PCT, isnotnull(cached), round((((memfree+(buffers+cached))/memtotal)*100),2), isnotnull(memtotal), round(((memfree/memtotal)*100),2))
EVAL-mem_used_percent = case(isnotnull(Real_total_MB), round((((Real_total_MB-Real_free_MB)/Real_total_MB)*100),2), isnotnull(cached), round((((((memtotal-(buffers+cached))-memfree))/memtotal)*100),2), isnotnull(memtotal), round((((memtotal-memfree)/memtotal)*100),2))

# for processes analysis
FIELDALIAS-process_name = Command as process_name
EVAL-process_cpu_used_percent = case(type=="TOP", pct_CPU)
EVAL-process_mem_used = case(type="TOP", case(OStype="AIX", (ResSet), OStype="Linux", (ResSet), OStype="Solaris", ResSize))

# Storage total_ops
EVAL-total_ops = case(type="DGXFER", value)

###########################################
#			nmon perf data JSON
###########################################

# New with 1.2.55, allows the performance data to be generated in json format

# This the indexed time json format
[nmon_data:json:indexed]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
CHARSET=UTF-8
INDEXED_EXTRACTIONS=json
KV_MODE=json
disabled=false
pulldown_type=true
TIME_FORMAT=%d-%m-%Y %H:%M:%S
TIMESTAMP_FIELDS=ZZZZ

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_data_json_hostoverride

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-nmon_data_json_sourcetypeoverride = nmon_data_json_sourcetypeoverride

# Leaving PUNCT enabled can impact indexing performance, and uses space
# For structured data, it has poor interest and shall be deactivated
ANNOTATE_PUNCT=false

# This is the extracted search time json format
[nmon_data:json:extracted]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
CHARSET=UTF-8
disabled=false
pulldown_type=true
TIME_FORMAT=%d-%m-%Y %H:%M:%S
TIME_PREFIX = \"ZZZZ\":\s"
KV_MODE=json

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_data_json_hostoverride

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-nmon_data_json_sourcetypeoverride = nmon_data_json_sourcetypeoverride

# Leaving PUNCT enabled can impact indexing performance, and uses space
# For structured data, it has poor interest and shall be deactivated
ANNOTATE_PUNCT=false

# For search heads, manage the KV_MODE from source for json indexed data
[source::perfdata:json:indexed]
KV_MODE=none

# For search heads, manage the KV_MODE from source for json extracted data
[source::perfdata:json:extracted]
KV_MODE=json

# Additional: In full extracted mode, we want 2 basic Nmon extracted at indexed time
TRANSFORMS-nmon_data_json_createindexed_time = nmon_data_json_createindexed_OStype, nmon_data_json_createindexed_type

###########################################
#			nmon processing stanza				#
###########################################

[nmon_processing]

TIME_FORMAT=%d-%m-%Y %H:%M:%S

###########################################
#			nmon config stanza					#
###########################################

[nmon_config]

BREAK_ONLY_BEFORE=CONFIG,
MAX_EVENTS=100000
NO_BINARY_CHECK=1
SHOULD_LINEMERGE=true
TIME_FORMAT=%d-%b-%Y:%H:%M
TIME_PREFIX=CONFIG,
TRUNCATE=0

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_config_hostoverride

##############################################
#			SYSLOG SPECIAL SECTIONS					#
##############################################

# These parameters are dedicated to the deployment of Nmon using syslog as the transport layer
# to forward Nmon Performance data from your end servers to your central syslog, and finally to Splunk

# Deploying Nmon with syslog requires additional configuration on search heads, and potentially indexers
# This also requires specific configuration on end clients (rsyslog/syslog-ng config, cron config, logrotate)

# Data being transferred through syslog is not csv structured data but key=value data
# each source of data is being in a temporary sourcetype for indexing time parsing to occur, then we will
# rewrite the sourcetypes to match standard sourcetypes being used by Core application

# depending on its origin (the source, normal vs syslog), the configuration on the sh head will apply the correct
# extraction parameters at search time.

# The nmon-logger package for Syslog deployment is available at:
# https://github.com/guilhemmarchand/nmon-logger

# Online guides for Syslog deployment:
# rsyslog: http://nmonsplunk.wikidot.com/documentation:installation:rsyslog
# syslog-ng: http://nmonsplunk.wikidot.com/documentation:installation:syslog-ng

####################
# Syslog to Splunk #
####################
#
# If you receive directly from syslog over TCP/UDP, use this sourcetype
# Create a dedicated UDP or TCP input with:
# index = nmon
# source = (leave optional or put anything you like)
# sourcetype = nmon:fromsyslog

# This sourcetype must manage incoming multi / or mono line event
# syslog timestamp is not being used for data time stamping (using nmon timestamp)

#
# Splunk to Splunk is not recommended, prefer having a Splunk instance installed on rsyslog collectors
# or on machine receiving rsyslog data, and generating local per host files
# Therefore, it is still possible to achieve it using a tcp/udp input using this sourcetype:

[nmon:fromsyslog]

BREAK_ONLY_BEFORE=timestamp="
MAX_EVENTS=100000
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
MAX_TIMESTAMP_LOOKAHEAD=26

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS = syslog-host

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-perfdata_fromsyslog = nmon_data_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_config
TRANSFORMS-configdata_fromsyslog = nmon_config_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_processing
TRANSFORMS-processingdata_fromsyslog = nmon_processing_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_collect
TRANSFORMS-collectdata_fromsyslog = nmon_collect_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_clean
TRANSFORMS-cleandata_fromsyslog = nmon_clean_fromsyslog_rewrite

#####################################
# Splunk to Splunk with syslog data #
#####################################

# These sourcetypes will be used when reading local files generated by syslog from remote syslog clients
# Each type of data must have its own file monitor.

# See inputs.conf.spec for more information, or read the online wiki manual pages for rsyslog / syslog-ng deployments

#
# For nmon performance data:
#

###################################################
# Set parameters for nmon_data coming from syslog #
###################################################

[nmon_data:fromsyslog]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
MAX_TIMESTAMP_LOOKAHEAD=26
KV_MODE=auto

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS = syslog-host

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-perfdata_fromsyslog = nmon_data_fromsyslog_rewrite

# For search heads, activate kvmode to auto for that source
[source::perfdata:syslog]
KV_MODE=auto

#
# For nmon configuration data:
#

#####################################################
# Set parameters for nmon_config coming from syslog #
#####################################################

[nmon_config:fromsyslog]
BREAK_ONLY_BEFORE=timestamp="
MAX_EVENTS=100000
NO_BINARY_CHECK=1
SHOULD_LINEMERGE=true
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
TRUNCATE=0

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS = syslog-host

# Rewrite sourcetype to standard nmon_config
TRANSFORMS-configdata_fromsyslog = nmon_config_fromsyslog_rewrite

#
# For nmon processing data:
#

[nmon_processing:fromsyslog]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
TIME_FORMAT=%d-%m-%Y %H:%M:%S
TIME_PREFIX=nmon2csv:
CHARSET=UTF-8
BREAK_ONLY_BEFORE=nmon2csv:

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS = syslog-host

# Rewrite sourcetype to standard nmon_processing
TRANSFORMS-processingdata_fromsyslog = nmon_processing_fromsyslog_rewrite

#
# For nmon collecting data:
#

[nmon_collect:fromsyslog]

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS = syslog-host

# Rewrite sourcetype to standard nmon_collect
TRANSFORMS-collectdata_fromsyslog = nmon_collect_fromsyslog_rewrite

#
# For nmon clean data:
#

[nmon_clean:fromsyslog]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
BREAK_ONLY_BEFORE=Starting nmon cleaning
TIME_FORMAT=%c
TIME_PREFIX=\w*\s
CHARSET=UTF-8

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS = syslog-host

# Rewrite sourcetype to standard nmon_clean
TRANSFORMS-cleandata_fromsyslog = nmon_clean_fromsyslog_rewrite
