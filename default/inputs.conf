##################################
#			nmon2csv stanza			#
##################################

# Source stanza for nmon2csv converter scripts
# Associated with the source stanza within props.conf
# Every nmon file present within the directory will be converted into csv files
# Splunk can manage

# NEW: Starting with v1.5.0, the main input monitor for nmon files runs in monitor instead of batch mode
# NEW: Starting with V1.5.07, you can manage gzip compressed files repositories (cold archives nmon data) without further configuration, just add your entry thats matches your *.nmon.gz files

[monitor://$SPLUNK_HOME/var/log/nmon/var/nmon_repository/*nmon]

disabled = false
followTail = 0
recursive = false
index = nmon
sourcetype = nmon_processing
crcSalt = <SOURCE>

# Since the release 1.3.0, the addons will use fifo files to minimize the CPU footprint of the nmon processing
# The following input script will consume the data produced by the fifo readers scripts

# This mode is currently not available on Solaris, for this OS the script will exit without doing any action

[script://./bin/fifo_consumer.sh]
disabled = false
index = nmon
interval = 60
source = fifo_consumer
sourcetype = nmon_processing

####################################################
#			nmon csv converted files indexing			#
####################################################

# Every file present within this directory will be indexed then deleted
# This section should not be modified under normal use

[batch://$SPLUNK_HOME/var/log/nmon/var/csv_repository/*nmon*.csv]

disabled = false
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_data
# source override: to prevent Metadata from having millions of entry, the source is overridden by default
# You can disable this for trouble shooting purposes if required
source = perfdata

# Starting with version 1.2.55, you can choose to generate the performance data in json format instead of legacy csv

# In json format, you can as well choose between 2 pre-configured sourcetypes:
# - "sourcetype = nmon_data:json:extracted" and "source = perfdata:json:extracted": Index the json data as raw data and extract the fields at search time, this saves storage at the price of lower performances
# - "sourcetype = nmon_data:json:indexed" and "source = perfdata:json:indexed": Index the json data as json indexed, best performances at the cost of more storage requirements

# Activating the json generation occurs in the props.conf level (create a local/props.conf and include the --json_output argument in data parsing)

[batch://$SPLUNK_HOME/var/log/nmon/var/json_repository/*nmon*.json]

disabled = false
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_data:json:extracted
source = perfdata:json:extracted

####################################################
#			nmon config files indexing
####################################################

# Every file present within this directory will be indexed then deleted
# This section should not be modified under normal use

[batch://$SPLUNK_HOME/var/log/nmon/var/config_repository/*nmon*.csv]

disabled = false
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_config
# source override: to prevent Metadata from having millions of entry, the source is overridden by default
# You can disable this for trouble shooting purposes if required
source = configdata

####################################################
#           nmon data collect
####################################################

# These input script sanza will run nmon and generates nmon file 
# to be exploited by Splunk

# For AIX / Linux / Solaris

# NEW: Starting with v1.5.0, the nmon_helper.sh main options can be managed by the nmon.conf file, and 
# the script won't kill anymore any running nmon but check for it and exit without no action if a PID is found
# In default configuration, check every 60 seconds that nmon is running

[script://./bin/nmon_helper.sh]
disabled = false
index = nmon
interval = 60
source = nmon_collect
sourcetype = nmon_collect

####################################################
#		    nmon cleaner
####################################################

# NEW with v1.5.0: A cleaner script is provided in Python and Perl to manage the cleaning of
# Nmon raw data files and csv perf and config files
# In normal circumstances, there should never be any csv files in csv_respository and config_repository, this action is off by default
# and unless you ask it, cleaner scripts won't do anything in these directories
# But if you want to prevent from running out of file handle or abnormal CPU usage in case of unexpected situation (eg. batch mode not working for example)
# you can use the "--cleancsv" switch (see the full help above)

# By default and without any option provided, cleaner scripts (nmon_cleaner.py and nmon_cleaner.pl) will:
# - Search for nmon files (*.nmon) in default nmon_repository and remove any file older than 24 hours

# Available options are:
# --cleancsv :Activate the purge of csv files from csv repository and config repository (see also options above)
# --maxseconds_csv <value> :Set the maximum file retention in seconds for csv data, every files older than this value will be permanently removed
# --maxseconds_nmon <value> :Set the maximum file retention in seconds for nmon files, every files older than this value will be permanently removed
# --approot <value> :Set a custom value for the Application root directory (default are: nmon / TA-nmon / PA-nmon)
# --csv_repository <value> :Set a custom location for directory containing csv data (default: csv_repository)
# --config_repository <value> :Set a custom location for directory containing config data (default: config_repository)
# --nmon_repository <value> :Set a custom location for directory containing nmon raw data (default: nmon_repository)
# --version :Show current program version

# NOTE: When managing larget set of cold Nmon data generated out of Splunk, it is recommended to set an higher custom value for csv max retention
# The default value is 10 minutes, in case of high load of Splunk, this can lead to delete data before it is being indexed.
# When Splunk generates the Nmon data, we assume csv volatile data should have indexed in 10 minutes max.

# NEW since Version 1.5.07, a frontal simple shell script is used to encapsulate both Python and Perl script
# If Python is not locally available, the Perl version will be automatically used

[script://./bin/nmon_cleaner.sh --cleancsv]
disabled = false
index = nmon
interval = 600
source = nmon_cleaner
sourcetype = nmon_clean

###########################################
#			SA-Eventgen
###########################################

# All these stanza are related to Eventgen configuration, for App testing purposes, Evengen will generate various sample data
# These settings can be safety ignored in Production systems as they will never affect anything as long as Eventgen inputs are not activated

[batch://$SPLUNK_HOME/var/log/nmon/eventgen/*sample.csv]

disabled = true
move_policy = sinkhole
recursive = false
crcSalt = <SOURCE>
index = nmon
sourcetype = nmon_data
