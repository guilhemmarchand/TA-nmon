# props.conf

##################################
#			nmon2csv stanza			#
##################################

# IMPORTANT: Since the V1.6.0, the configuration uses the nmon2csv.sh wrapper that will call nmon2csv.py (prefered choice) or nmon2csv.pl if appropriated
# You can still force the configuration to use the Python or Perl converter (but the nmon2csv.sh wrapper will do the choice for you, enforcing the choice is not required anymore)

# *** BE UPGRADE RESILIENT: *** Copy this stanza to your local/props.conf to prevent futur upgrades from overwritting your setting 

# To force the use of the Python converter, set:
# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.py

# To force the use of the Python converter, set:
# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.pl

# This source stanza will be called by the archive processor to convert NMON raw data into csv files
# Splunk can manage. See inputs.conf for the associated monitor

# To force nmon2csv parsers to always consider nmon files as realtime data, you can set the option "--mode realtime":
# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh --mode realtime
# This is normally not necessary as parsers will automatically detect if we are dealing with realtime data

# host name definition: by default, nmon2csv parsers will use the value returned by nmon for the host name definition. (available in the config section - AAA,host)
# If you want to have hosts using their fully qualified domain name (fqdn) instead of the nmon hostname value, add the option "--use_fqdn" in the source stanza definition
# the value of host name will be equivalent to the "hostname -f" command on the host.
# **CAUTION**: Do use this option when managing nmon data generated out of Splunk (central repositories) or all the data being ingested will be identified as coming the host
# that managed the nmon data
# This option must be used on host that are only managing their own nmon data.

# To activate fqdn:
# unarchive_cmd = $SPLUNK_HOME/bin/splunk cmd $SPLUNK_HOME/etc/apps/nmon/bin/nmon2csv.sh --use_fqdn

### CSV indexing versus JSON indexing ###

# New since version 1.2.55 of the addons, you can choose to generate the performance data into json data instead of legacy csv data

# This can be activated in a custom local/props.conf with the addition of the "--json_output" argument, example:

# unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh --mode realtime --json_output

# In inputs.conf, you can as well choose between 2 ways of indexing the json data:

# - "sourcetype = nmon_data:json:extracted" and "source = perfdata:json:extracted": Index the json data as raw data and extract the fields at search time, this saves storage at the price of lower performances
# - "sourcetype = nmon_data:json:indexed" and "source = perfdata:json:indexed": Index the json data as json indexed, best performances at the cost of more storage requirements

# The default of inputs.conf used the search time extracted mode, saves storage but provides lower performance in raw SPL searches

# See the following documentation for more information about implications in term of performance, volume of data and licensing costs, as well as long term storage requirements

# http://ta-nmon.readthedocs.io/en/latest/json_indexing.html

# Finally, the json mode is available currently only for the Python parser, this will have no effects on a server using Perl parser

[source::.../*.nmon]
invalid_cause = archive
unarchive_cmd = $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh --mode realtime
sourcetype = nmon_processing
NO_BINARY_CHECK = true

# To manage repositories archives of cold nmon files (add you own for other compressed formats)
[source::.../*.nmon.gz]
invalid_cause = archive
unarchive_cmd = gunzip | $SPLUNK_HOME/etc/apps/TA-nmon/bin/nmon2csv.sh
sourcetype = nmon_processing
NO_BINARY_CHECK = true

###########################################
#			nmon converted csv stanza			#
###########################################

# This sourcetype stanza will be used to index nmon csv converted data
# Every generated csv file will contain a CSV header used by Splunk to identify fields

[nmon_data]
FIELD_DELIMITER=,
FIELD_QUOTE="
HEADER_FIELD_LINE_NUMBER=1

# your settings
INDEXED_EXTRACTIONS=csv
NO_BINARY_CHECK=1
SHOULD_LINEMERGE=false
TIMESTAMP_FIELDS=ZZZZ
TIME_FORMAT=%d-%m-%Y %H:%M:%S

# set by detected source type
KV_MODE=none
pulldown_type=true

# Leaving PUNCT enabled can impact indexing performance, and uses space
# For structured data, it has poor interest and shall be deactivated
ANNOTATE_PUNCT=false

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_data_hostoverride

#
# search time extractions: The following stanzas will occur at search time only
#

########################################
# CIM normalization / NMON extractions #
########################################

# When applicable, be CIM compliant

# Various CIM
FIELDALIAS-dest = host as dest
EVAL-hypervisor_id = if(isnotnull(frameID), frameID, serialnum)

#
# Non CIM and Power arch common, unify AIX and PowerLinux stats to a common model at search extraction time
#

# For Power arch micro-partitions (AIX & PowerLinux)
EVAL-lpar_vp_usage = case(OStype=="AIX", round(((VP_User_PCT+VP_Sys_PCT+VP_Wait_PCT+VP_Idle_PCT)*virtualCPUs/100),3), OStype=="Linux", round(PhysicalCPU,3))
EVAL-lpar_vp_usage_PCT = case(OStype=="AIX", round((VP_User_PCT+VP_Sys_PCT+VP_Wait_PCT+VP_Idle_PCT),2), OStype=="Linux", round((PhysicalCPU/partition_active_processors*100),2))

# For Power arch micro-partitions Pool statistics (AIX & PowerLinux)
# Note: For PowerLinux, at the perf collection startup the pool usage may report almost 100% usage which is incorrect, filtering with pool_idle_time>"0.1" will prevent from loading these data

EVAL-lpar_pool_vp_usage = case(OStype=="AIX", round((poolCPUs-PoolIdle),3), OStype="Linux", case(pool_idle_time>"0.1" AND pool_idle_time<(pool_capacity/100), round((((pool_capacity/100)-pool_idle_time)/smt_mode),3)) )
EVAL-lpar_pool_vp_usage_PCT = case(OStype=="AIX", round(((poolCPUs-PoolIdle)*100/poolCPUs),2), OStype="Linux", case(pool_idle_time>"0.1" AND pool_idle_time<(pool_capacity/100), round((((pool_capacity/100)-pool_idle_time)/smt_mode)/(pool_capacity/100)*100,2)) )

# Normalize capacity information
EVAL-lpar_pool_capacity = case(OStype=="AIX", poolCPUs, OStype="Linux", (pool_capacity/100))
EVAL-lpar_vp_capacity = case(OStype=="AIX", virtualCPUs, OStype="Linux", partition_active_processors)
EVAL-lpar_entitled_capacity = case(OStype=="AIX", entitled, OStype="Linux", partition_entitled_capacity)

# CIM normalization for cpu metrics

EVAL-cpu_load_percent = case(match(type,"CPU.*"), (Sys_PCT+User_PCT+Wait_PCT), type=="TOP", pct_CPU, type=="LPAR", round((VP_User_PCT+VP_Sys_PCT+VP_Wait_PCT+VP_Idle_PCT),2), OStype=="Linux", round((PhysicalCPU/partition_active_processors*100),2) )
FIELDALIAS-cpu_load_user = User_PCT as cpu_user_percent
# Non CIM, but provides good visibility on these fields, and this looks like logical
FIELDALIAS-cpu_load_sys = Sys_PCT as cpu_sys_percent
FIELDALIAS-cpu_load_wait = Wait_PCT as cpu_wait_percent
FIELDALIAS-cpu_load_idle = Idle_PCT as cpu_idle_percent

# Non CIM: provide the same fields evaluation in logical CPU units
EVAL-cpu_load_logicalcpus = case(match(type,"CPU_ALL"), ((Sys_PCT+User_PCT+Wait_PCT)/100*logical_cpus))
EVAL-cpu_user_logicalcpus = case(match(type,"CPU_ALL"), (User_PCT/100*logical_cpus))
EVAL-cpu_wait_logicalcpus = case(match(type,"CPU_ALL"), (Wait_PCT/100*logical_cpus))
EVAL-cpu_idle_logicalcpus = case(match(type,"CPU_ALL"), (Idle/100*logical_cpus))

# CIM normalization for common memory metrics

# Total memory available in MB (for AIX / Linux / Solaris)
EVAL-mem = case(isnotnull(Real_total_MB), Real_total_MB, isnotnull(memtotal), memtotal)
EVAL-mem_free = case(isnotnull(Real_free_MB), Real_free_MB, isnotnull(memfree), memfree)
EVAL-mem_used = case(type=="MEM", (case(isnotnull(Real_total_MB), (Real_total_MB-Real_free_MB), isnotnull(memtotal), (memtotal-memfree))), type="TOP", case(OStype="AIX", ((ResData+ResText)), OStype="Linux", (ResSet), OStype="Solaris", ResSize))

# Non CIM: For Linux, create the effective mem_used and mem_free, effective means memory less buffers and cached that can be free by the kernel if it required
# Volume of buffers and cached are by default accounted in the memory being used, therefore this is can be considered as a level memory available for system and application needs
# For other OS, these fields will be created for convenient use but are they are equal to previously calculated fields
EVAL-mem_used_effective = case(type=="MEM", (case(isnotnull(Real_total_MB), (Real_total_MB-Real_free_MB), isnotnull(cached), ((memtotal-(buffers+cached))-memfree), isnotnull(memtotal), memtotal-memfree)))
EVAL-mem_free_effective = case(isnotnull(Real_free_MB), Real_free_MB, isnotnull(cached), (memfree+(buffers+cached)), isnotnull(memtotal), memfree)

# Evaluate at the props level the percentile versions of memory statistics
EVAL-mem_used_PCT = case(isnotnull(Real_total_MB), round((((Real_total_MB-Real_free_MB)/Real_total_MB)*100),2), isnotnull(memtotal), round((((memtotal-memfree)/memtotal)*100),2))
EVAL-mem_free_PCT = case(isnotnull(Real_Free_PCT), Real_Free_PCT, isnotnull(memtotal), round(((memfree/memtotal)*100),2))
EVAL-mem_used_effective_PCT = case(isnotnull(Real_total_MB), round((((Real_total_MB-Real_free_MB)/Real_total_MB)*100),2), isnotnull(cached), round((((((memtotal-(buffers+cached))-memfree))/memtotal)*100),2), isnotnull(memtotal), round((((memtotal-memfree)/memtotal)*100),2))
EVAL-mem_free_effective_PCT = case(isnotnull(Real_Free_PCT), Real_Free_PCT, isnotnull(cached), round((((memfree+(buffers+cached))/memtotal)*100),2), isnotnull(memtotal), round(((memfree/memtotal)*100),2))

# Non CIM: For Linux, eval cached, buffers, active and inactive in PCT
EVAL-cached_PCT = case(isnotnull(cached), round(((cached/memtotal)*100),2))
EVAL-buffers_PCT = case(isnotnull(buffers), round(((buffers/memtotal)*100),2))
EVAL-active_PCT = case(isnotnull(active), round(((active/memtotal)*100),2))
EVAL-inactive_PCT = case(isnotnull(inactive), round(((inactive/memtotal)*100),2))

# Total virtual memory available in MB (for AIX / Linux / Solaris)
EVAL-swap = case(isnotnull(Virtual_total_MB), Virtual_total_MB, isnotnull(swaptotal), swaptotal)
EVAL-swap_free = case(isnotnull(Virtual_free_MB), Virtual_free_MB, isnotnull(swapfree), swapfree)
EVAL-swap_used = case(isnotnull(Virtual_total_MB), (Virtual_total_MB-Virtual_free_MB), isnotnull(swaptotal), (swaptotal-swapfree))

# Non CIM
EVAL-swap_free_PCT = case(isnotnull(Virtual_free_PCT), Virtual_free_PCT, isnotnull(swapfree), round(((swapfree/swaptotal)*100),2))
EVAL-swap_used_PCT = case(isnotnull(Virtual_total_MB), round((((Virtual_total_MB-Virtual_free_MB)/Virtual_total_MB)*100),2), isnotnull(swaptotal), round((((swaptotal-swapfree)/swaptotal)*100),2))

# For Linux, consider effective swap using over the volume of swapcached
EVAL-swap_free_effective = case(isnotnull(Virtual_free_MB), Virtual_free_MB, isnotnull(swapcached), (swapfree+swapcached), isnotnull(swapfree), swapfree)
EVAL-swap_used_effective = case(isnotnull(Virtual_total_MB), (Virtual_total_MB-Virtual_free_MB), isnotnull(swapcached), ((swaptotal-swapfree)-swapcached), isnotnull(swaptotal), (swaptotal-swapfree))
EVAL-swap_free_effective_PCT = case(isnotnull(Virtual_free_PCT), Virtual_free_PCT, isnotnull(swapcached), round((((swapfree+swapcached)/swaptotal)*100),2), isnotnull(swapfree), round(((swapfree/swaptotal)*100),2))
EVAL-swap_used_effective_PCT = case(isnotnull(Virtual_total_MB), round((((Virtual_total_MB-Virtual_free_MB)/Virtual_total_MB)*100),2), isnotnull(swapcached), round((((((swaptotal-swapfree)-swapcached))/swaptotal)*100),2), isnotnull(swaptotal), round((((swaptotal-swapfree)/swaptotal)*100),2))

# Non CIM
EVAL-swapcached_PCT = case(isnotnull(swapcached), round(((swapcached/swaptotal)*100),2))

# CIM normalization for common network statistics
EVAL-thruput = case(type=="NET", (value*1000))
EXTRACT-dvc_name = (?<dvc_name>\w*)\-(?<direction_pattern>read|write)\w*
FIELDALIAS-dvc = dvc_name as dvc
EVAL-direction = case(direction_pattern=="read", "inbound", direction_pattern=="write", "outbound")
EVAL-bytes_in = case(type=="NET", case(direction_pattern=="read", (value*1000)) )
EVAL-bytes_out = case(type=="NET", case(direction_pattern=="write", (value*1000)) )
EVAL-bytes = case(case(type=="NET", direction_pattern=="read"), (value*1000), case(type=="NET", direction_pattern=="write"), (value*1000) )
EVAL-packets_in = case(type=="NETPACKET", case(direction_pattern=="read", (value*1000)) )
EVAL-packets_out = case(type=="NETPACKET", case(direction_pattern=="write", (value*1000)) )
EVAL-packets = case(case(type=="NETPACKET", direction_pattern=="read"), (value*1000), case(type=="NETPACKET", direction_pattern=="write"), (value*1000) )

# CIM for ApplicationState, linked to TOP monitors
FIELDALIAS-Command = Command as process
FIELDALIAS-process_id = PID as process_id
EVAL-user = case(type=="TOP", if(isnotnull(username), username, "undefined"))

# CIM normalization for storage metrics (JFSFILE and nmon external DF_STORAGE)
EVAL-mount = case(match(type, "JFS\w*"), device, match(type, "DF_\w*"), mount)
EVAL-storage = case(type=="DF_STORAGE", round((blocks/1024), 2))
EVAL-storage_free = case(type=="DF_STORAGE", round(((blocks-Used)/1024), 2))
EVAL-storage_free_percent = case(type=="DF_STORAGE", (100-Use_pct), type=="JFSFILE", (100 - value))
EVAL-storage_used = case(type=="DF_STORAGE", round((Used/1024), 2))
EVAL-storage_used_percent = case(type=="DF_STORAGE", Use_pct, type=="JFSFILE", value)

# For inodes, non CIM
EVAL-storage_inodes = case(type=="DF_INODES", Inodes)
EVAL-storage_free_inodes = case(type=="DF_INODES", IFree)
EVAL-storage_free_inodes_percent = case(type=="DF_INODES", (100-IUse_pct))
EVAL-storage_used_inodes = case(type=="DF_INODES", IUsed, type=="JFSINODES", value)
EVAL-storage_used_inodes_percent = case(type=="DF_INODES", IUse_pct, type=="JFSINODES", (100-value))

# Non CIM for disk extended statistics (DG*)
EVAL-disk_backlog_time_ms = case(type=="DGBACKLOG", value)
EVAL-disk_busy_time_pct = case(type=="DGBUSY", value)
EVAL-disk_in_flight_io = case(type=="DGINFLIGHT", value)
EVAL-disk_time_spent_for_io_ms = case(type=="DGIOTIME", value)
EVAL-disk_read_KB_per_sec = case(type=="DGREAD", value)
EVAL-disk_merge_read_sec = case(type=="DGREADMERGE", value)
EVAL-disk_read_sec = case(type=="DGREADS", value)
EVAL-disk_read_service_time_ms = case(type=="DGREADSERV", value)
EVAL-disk_block_size_KB = case(type=="DGSIZE", value)
EVAL-disk_write_KB_per_sec = case(type=="DGWRITE", value)
EVAL-disk_merge_write_sec = case(type=="DGWRITEMERGE", value)
EVAL-disk_write_sec = case(type=="DGWRITES", value)
EVAL-disk_write_service_time_ms = case(type=="DGWRITESERV", value)
EVAL-disk_total_iops = case(type=="DGXFER", value)

#
# NMON EXTERNAL
#

# uptime from nmon_external

# extract the load average
EXTRACT-uptime_loadaverage = load[_\-\s]average:\s*(?<load_average_1min>[\d|\.]*);\s*(?<load_average_5min>[\d|\.]*);\s*(?<load_average_15min>[\d|\.]*)

# uptime
# Sic !!!
EXTRACT-uptime_days = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_days>\d*)[\s|_]*day[s|,|;]*[\s|_]*(?<uptime_dayshour>\d*):(?<uptime_daystime>\d*)
EXTRACT-uptime_days_with_min = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_days>\d*)[\s|_]*day[s|,|;]*[\s|_]*(?<uptime_daysmin>\d*)[\s|_]min
EXTRACT-uptime_minutes = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_minutes>\d*)[\s|_]*min[\s|_]*[,|;]
EXTRACT-uptime_hours = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_hours>\d*:\d*)[\s|_]*[,|;]
EXTRACT-uptime_hours_hours = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*(?<uptime_hours_hours>\d*):\d*[\s|_]*[,|;]
EXTRACT-uptime_hours_minutes = \s{0,}[\d|\w|:]*[\s|_]*up[\s|_]*\d*:(?<uptime_hours_minutes>\d*)[\s|_]*[,|;]
EVAL-uptime_fromdays = (uptime_days*86400)
EVAL-uptime_fromminutes = (uptime_minutes*60)
EVAL-uptime_fromhours = (uptime_hours_hours*60*60) + (uptime_hours_minutes*60)
EVAL-uptime = case(isnotnull(uptime_daysmin), ((uptime_days*86400)+(uptime_daysmin*60)), isnotnull(uptime_days), ((uptime_days*86400)+(uptime_dayshour*3600)+(uptime_daystime*60)), isnotnull(uptime_minutes), (uptime_minutes*60), isnotnull(uptime_hours), (uptime_hours_hours*60*60) + (uptime_hours_minutes*60) )

### ITSI normalization ###

# ITSI has its own data model

# from type-PROC, the number of threads waiting for available processors
FIELDALIAS-wait_threads_count = Runnable as wait_threads_count

# for memory analysis
EVAL-mem_free_percent = case(isnotnull(Real_Free_PCT), Real_Free_PCT, isnotnull(cached), round((((memfree+(buffers+cached))/memtotal)*100),2), isnotnull(memtotal), round(((memfree/memtotal)*100),2))
EVAL-mem_used_percent = case(isnotnull(Real_total_MB), round((((Real_total_MB-Real_free_MB)/Real_total_MB)*100),2), isnotnull(cached), round((((((memtotal-(buffers+cached))-memfree))/memtotal)*100),2), isnotnull(memtotal), round((((memtotal-memfree)/memtotal)*100),2))

# for processes analysis
FIELDALIAS-process_name = Command as process_name
EVAL-process_cpu_used_percent = case(type=="TOP", pct_CPU)
EVAL-process_mem_used = case(type="TOP", case(OStype="AIX", (ResSet), OStype="Linux", (ResSet), OStype="Solaris", ResSize))

# Storage total_ops
EVAL-total_ops = case(type="DGXFER", value)

###########################################
#			nmon perf data JSON
###########################################

# New with 1.2.55, allows the performance data to be generated in json format

# This the indexed time json format
[nmon_data:json:indexed]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
CHARSET=UTF-8
INDEXED_EXTRACTIONS=json
KV_MODE=json
disabled=false
pulldown_type=true
TIME_FORMAT=%d-%m-%Y %H:%M:%S
TIMESTAMP_FIELDS=ZZZZ

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_data_json_hostoverride

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-nmon_data_json_sourcetypeoverride = nmon_data_json_sourcetypeoverride

# Leaving PUNCT enabled can impact indexing performance, and uses space
# For structured data, it has poor interest and shall be deactivated
ANNOTATE_PUNCT=false

# This is the extracted search time json format
[nmon_data:json:extracted]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
CHARSET=UTF-8
disabled=false
pulldown_type=true
TIME_FORMAT=%d-%m-%Y %H:%M:%S
TIME_PREFIX = \"ZZZZ\":\s"
KV_MODE=json

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_data_json_hostoverride

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-nmon_data_json_sourcetypeoverride = nmon_data_json_sourcetypeoverride

# Leaving PUNCT enabled can impact indexing performance, and uses space
# For structured data, it has poor interest and shall be deactivated
ANNOTATE_PUNCT=false

# For search heads, manage the KV_MODE from source for json indexed data
[source::perfdata:json:indexed]
KV_MODE=none

# For search heads, manage the KV_MODE from source for json extracted data
[source::perfdata:json:extracted]
KV_MODE=json

# Additional: In full extracted mode, we want 2 basic Nmon extracted at indexed time
TRANSFORMS-nmon_data_json_createindexed_time = nmon_data_json_createindexed_OStype, nmon_data_json_createindexed_type

###########################################
#			nmon processing stanza
###########################################

[nmon_processing]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_PREFIX=^
TIME_FORMAT=%d-%m-%Y %H:%M:%S
MAX_TIMESTAMP_LOOKAHEAD=19
LINE_BREAKER=([\n\r]+)\d{2}-\d{2}-\d{4}\s\d{2}:\d{2}:\d{2}
TRUNCATE=999999

# Deactivate KV
KV_MODE=none

# For TA-nmon
EXTRACT-splunk_home = (?i).+\w*\sRoot\sDirectory\s\(\$SPLUNK_HOME\)\:\s{0,}(?P<splunk_home>[a-zA-Z0-9\/\\\-\_\.\:]+)\s

# For nmon-logger
EXTRACT-nmon_home = (?i).+\w*\sRoot\sDirectory\s\(\$NMON_VAR\)\:\s{0,}(?P<nmon_var>[a-zA-Z0-9\/\\\-\_\.\:]+)\s
EXTRACT-operating_system = (?i).+Guest\sOperating\sSystem\:\s{0,}(?P<operating_system>[a-zA-Z0-9]+)\s
EXTRACT-addon_type = (?i).*addon\s*type:\s.*\/(?<addon_type>[a-zA-Z0-9|\-\_]+)\s
EXTRACT-addon_version = (?i).+addon\sversion\:\s{0,}(?P<addon_version>[a-zA-Z0-9\.]+)\s
EXTRACT-python_version = (?i).+Python\sversion\:\s{0,}(?P<python_version>[a-zA-Z0-9\.]+)\s
EXTRACT-perl_version = (?i).+Perl\sversion\:\s{0,}(?P<perl_version>[a-zA-Z0-9\.]+)\s
EVAL-converter_inuse = case(isnotnull(python_version), "Python", isnotnull(perl_version), "Perl")
EVAL-interpreter_version = case(isnotnull(python_version), python_version, isnotnull(perl_version), perl_version)
EXTRACT-nmon2csv_version = (?i).+nmon2csv\sversion\:\s{0,}(?P<nmon2csv_version>[0-9\.]+)\s
EXTRACT-hostname = (?i).+HOSTNAME\:\s{0,}(?P<hostname>[a-zA-Z0-9\-\_\.]+)
EXTRACT-nbr_lines = (?i).+Reading\sNMON\sdata:\s{0,}(?P<nbr_lines>\d+)\slines
EXTRACT-size_in_bytes = (?i).+lines\s(?P<size_in_bytes>\d+)\sbytes
EXTRACT-elapsed_in_seconds = (?i).+Elapsed\stime\swas\:\s{0,}(?P<elapsed_in_seconds>\d+\.\d+)\sseconds
EXTRACT-Nmon_version = (?i).+NMON\sVERSION\:\s{0,}(?P<Nmon_version>[a-zA-Z0-9\-\_\.\s]+)\s
EXTRACT-Time_of_Nmon_data = (?i).+TIME\sof\sNmon\sData\:\s{0,}(?P<Time_of_Nmon_Data>[0-9\:\.]+)\s
EXTRACT-Date_of_Nmon_data = (?i).+DATE\sof\sNmon\sData\:\s{0,}(?P<Date_of_Nmon_Data>[a-zA-Z0-9\-\/]+)\s
EXTRACT-INTERVAL = (?i).+INTERVAL\:\s{0,}(?P<INTERVAL>\d+)\s
EXTRACT-SNAPSHOTS = (?i).+SNAPSHOTS\:\s{0,}(?P<SNAPSHOTS>\d+)\s
EXTRACT-logical_cpus = (?i).+logical_cpus\:\s{0,}(?P<logical_cpus>\d+)\s
EXTRACT-virtual_cpus = (?i).+virtual_cpus\:\s{0,}(?P<virtual_cpus>\d+)\s
EXTRACT-Nmon_ID = (?i).+NMON\sID\:\s{0,}(?P<Nmon_ID>[a-zA-Z0-9\-\:\,\_\.]+)\s

###########################################
#			nmon config stanza					#
###########################################

[nmon_config]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_PREFIX=^CONFIG,
TIME_FORMAT=%d-%b-%Y:%H:%M.%S
LINE_BREAKER=([\r\n]+)CONFIG,\d{2}-\w{3}-\d{4}:\d{2}:\d{2}\.\d{2},
TRUNCATE=0
MAX_EVENTS=100000
MAX_TIMESTAMP_LOOKAHEAD=30

# Deactivate KV
KV_MODE = none

EXTRACT-hostname = (?i)AAA,host,(?P<hostname>[a-zA-Z0-9\-\_\.]+)\s

# Overwritting default host field based on event data for nmon_data sourcetype (useful when managing Nmon central shares)
TRANSFORMS-hostfield=nmon_config_hostoverride

# Perform basic extractions
# Full extractions can be performed by calling associated macros, or using the data model
EXTRACT-AAA_OS = (?i),OS,(?P<AAA_OS>[^,]+)\s{0,}
EXTRACT-AIX_LEVEL = AAA,AIX,(?P<AIX_LEVEL>[a-zA-Z0-9\-\_\.]+)\s
EXTRACT-Linux_LEVEL = AAA,OS,Linux,(?P<Linux_LEVEL>[^,]+),
EXTRACT-Solaris_LEVEL = AAA,OS,Solaris,(?P<Solaris_LEVEL>[^,]+),
EVAL-OStype = case(AAA_OS == "Linux", "Linux", AAA_OS == "Solaris", "Solaris", isnotnull(AIX_LEVEL), "AIX", isnull(AAA_OS), "Unknown")

# When applicable, be CIM compliant

FIELDALIAS-dest = host as dest
EXTRACT-AAA_serial = AAA,SerialNumber,(?<AAA_serial>\w*)
EVAL-serial = if(isnotnull(AAA_serial), AAA_serial, host)
EVAL-hypervisor_id = if(isnotnull(AAA_serial), AAA_serial, hostname)
EVAL-hypervisor = if(isnotnull(AAA_serial), AAA_serial, hostname)

# family
EXTRACT-AIX_processor_type = BBB\w*,[^,]+,lsconf,\"Processor\sType:\s(?<AIX_processor_type>\w*)\"
EXTRACT-Linux_processor_type = AAA,OS,Linux,[^,]+,[^,]+,(?<Linux_processor_type>\w*)
EXTRACT-Solaris_processor_type = BBB.+,[0-9].+psrinfo\s\-pv,\"\s+(?P<Solaris_processor_type>[\w\-\_]*)\s*\(.+\"
EVAL-family = case(isnotnull(Linux_LEVEL), Linux_processor_type, isnotnull(Solaris_LEVEL), Solaris_processor_type, isnotnull(AIX_LEVEL), AIX_processor_type)

# Vendor / Product
EXTRACT-Linux_release_distribution = BBB\w*,[^,]*,.+etc+.release,\"(?!LSB_VERSION|DISTRIB|NAME|ID|VERSION)(?P<Linux_release_distribution>[^,]+)\"
EXTRACT-Linux_lsb_distribution = BBB\w*,[^,]*,lsb_release,\"Description:\s*(?<Linux_lsb_distribution>[^,]*)\"
EVAL-Linux_distribution = if(isnotnull(Linux_lsb_distribution), Linux_lsb_distribution, Linux_release_distribution)
EXTRACT-Solaris_version = BBB\w*,[^,]*,.+etc+.release,\"\s+(?P<Solaris_version>Oracle\s*Solaris\s[^\"]*)\"
EVAL-AIX_version = case(isnotnull(AIX_LEVEL), "IBM AIX" + " " + AIX_LEVEL)

EVAL-vendor_product = case(isnotnull(Linux_LEVEL), (if(isnotnull(Linux_lsb_distribution), Linux_lsb_distribution, Linux_release_distribution)), isnotnull(Solaris_LEVEL), Solaris_version, isnotnull(AIX_LEVEL), "IBM AIX " + 'AIX_LEVEL' )
EVAL-version = case(isnotnull(Linux_LEVEL), Linux_LEVEL, isnotnull(Solaris_LEVEL), Solaris_LEVEL, isnotnull(AIX_LEVEL), AIX_LEVEL )
EVAL-os = case(isnotnull(Linux_LEVEL), (if(isnotnull(Linux_lsb_distribution), Linux_lsb_distribution, Linux_release_distribution)), isnotnull(Solaris_LEVEL), Solaris_version, isnotnull(AIX_LEVEL), "IBM AIX " + 'AIX_LEVEL' )

# CPU
EXTRACT-AIX_virtualcpus = BBB\w*,[^,]+,lparstat\s-i,\"Online\sVirtual\sCPUs\s*:\s*(?<AIX_virtualcpus>\d*)\"
EXTRACT-cpu_cores_position1 = AAA,cpus,(?P<cpu_cores_position1>\d+)
EXTRACT-cpu_cores_position2 = AAA,cpus,\d+,(?P<cpu_cores_position2>\d+)
EVAL-cpu_cores_combo = (AIX_virtualcpus+" / "+cpu_cores_position2)

EVAL-cpu_cores = if(isnotnull(AIX_virtualcpus), AIX_virtualcpus, cpu_cores_position1)
EVAL-cpu_count = case(isnotnull(AIX_LEVEL), AIX_virtualcpus, isnotnull(Solaris_LEVEL), cpu_cores_position1, isnotnull(Linux_LEVEL), cpu_cores_position1)

EXTRACT-Solaris_processor_clockspeed = BBB.+,[0-9].+psrinfo\s\-pv,.+clock\s(?P<Solaris_processor_clockspeed>[\d\.]*)\s*\w*\)\"
EXTRACT-Linux_processor_clockspeed_1 = AAA,[^,]+,MHz,(?P<Linux_processor_clockspeed_1>[\d\.]*)
EXTRACT-Linux_processor_clockspeed_2 = BBB\w*,[^,]+,/proc/cpuinfo,\"cpu\s*MHz\s*:\s*(?<Linux_processor_clockspeed_2>[\d\.]*)\"
EXTRACT-Linux_processor_clockspeed_3 = BBB\w*,[^,]+,/proc/cpuinfo,\"clock\s{0,}:\s{0,}(?<Linux_processor_clockspeed_3>[\d|\.]*)\s{0,}MHz\"
EXTRACT-AIX_processor_clockspeed = BBBP,[^,]+,lsconf,"Processor\sClock\sSpeed:\s*(?<AIX_processor_clockspeed>[\d\.]*)\sMHz
EVAL-cpu_mhz = case(isnotnull(Linux_LEVEL), case(isnotnull(Linux_processor_clockspeed_1), Linux_processor_clockspeed_1, isnotnull(Linux_processor_clockspeed_2), Linux_processor_clockspeed_2, isnotnull(Linux_processor_clockspeed_3), Linux_processor_clockspeed_3), isnotnull(Solaris_LEVEL), Solaris_processor_clockspeed, isnotnull(AIX_LEVEL), AIX_processor_clockspeed)

EXTRACT-Linux_realmemory_KB = BBB\w*,[^,]*,/proc/meminfo,\"MemTotal:\s*(?<Linux_realmemory_KB>\d*)\s*kB\"
EXTRACT-Solaris_realmemory_KB = BBB\w*,[^,]*,prtdiag,\"Memory\s*size:\s*(?<Solaris_realmemory_MB>\d*)\sMegabytes\"
EXTRACT-AIX_realmemory_online_MB = BBB\w*,[^,]*,online\s*Memory,(?<AIX_realmemory_online_MB>\d*)
EXTRACT-AIX_realmemory_lsconf_MB = BBB\w*,[^,]*,lsconf,\"Memory\s*Size:\s*(?<AIX_realmemory_lsconf_MB>\d*)\s*MB\"
EVAL-mem = case(isnotnull(Linux_LEVEL), round((Linux_realmemory_KB/1024),0), isnotnull(Solaris_LEVEL), Solaris_realmemory_MB, isnotnull(AIX_realmemory_online_MB), if(isnotnull(AIX_realmemory_online_MB), AIX_realmemory_online_MB, AIX_realmemory_lsconf_MB) )

# Not in CIM
EXTRACT-Linux_swapmemory_KB = BBB\w*,[^,]*,/proc/meminfo,\"SwapTotal:\s*(?<Linux_swapmemory_KB>\d*)\s*kB\"

# Processor extraction
EXTRACT-AIX_processor = BBB\w*,[0-9]*,lsconf,\"Processor\s*Type:\s*(?P<AIX_processor>[^\"]*)\"
EXTRACT-Linux_arch = BBB\w*,[0-9]*,lscpu,\"(Architecture)\s*:\s+(?P<Linux_arch>[^\"]*)\"
EXTRACT-Linux_processor_lscpu = BBB\w*,[0-9]*,lscpu,\"(Model\sname|Model|Vendor\sID)\s*:\s*(?P<Linux_processor_lscpu>[^\"]*)\"
EXTRACT-Linux_processor_cpuinfo = BBB\w*,[0-9].*cpuinfo,\"(model\sname)\s*:\s*(?P<Linux_processor_cpuinfo>[^\"]*)\"
EXTRACT-Solaris_processor_primary = BBB.+,[0-9].+psrinfo\s\-pv,\"\s+(?P<Solaris_processor_primary>.+)\s*\(.+clock.+\"
EXTRACT-Solaris_processor_alt = BBB.+,[0-9].+prtdiag,\"0\s*(?<Solaris_processor_clock_alt>[\d|\.]*\s*\w*)\s*(?P<Solaris_processor_alt>[\w|-]*)\s*
EVAL-processor = case(isnotnull(AIX_LEVEL), AIX_processor, AAA_OS == "Solaris", case(isnotnull(Solaris_processor_primary), Solaris_processor_primary, isnotnull(Solaris_processor_alt), Solaris_processor_alt), AAA_OS == "Linux", case(isnotnull(Linux_processor_lscpu), Linux_processor_lscpu + " (arch: " + Linux_arch + ")" , isnull(Linux_processor_lscpu), Linux_processor_cpuinfo) )

# uptime (available for Linux hosts only)
# Sic !!!
EXTRACT-uptime_days = BBB\w*,[^,]+,uptime,\"\s*[\d|\w|:]*\s*up\s*(?<uptime_days>\d*)\s*day[s|,]*\s*(?<uptime_dayshour>\d*):(?<uptime_daystime>\d*)
EXTRACT-uptime_days_with_min = BBB\w*,[^,]+,uptime,\"\s*[\d|\w|:]*\s*up\s*(?<uptime_days>\d*)\s*day[s|,]*\s*(?<uptime_daysmin>\d*)\smin
EXTRACT-uptime_minutes = BBB\w*,[^,]+,uptime,\"\s*[\d|\w|:]*\s*up\s*(?<uptime_minutes>\d*)\s*min\s*,
EXTRACT-uptime_hours = BBB\w*,[^,]+,uptime,\"\s*[\d|\w|:]*\s*up\s*(?<uptime_hours>\d*:\d*)\s*,
EXTRACT-uptime_hours_hours = BBB\w*,[^,]+,uptime,\"\s*[\d|\w|:]*\s*up\s*(?<uptime_hours_hours>\d*):\d*\s*,
EXTRACT-uptime_hours_minutes = BBB\w*,[^,]+,uptime,\"\s*[\d|\w|:]*\s*up\s*\d*:(?<uptime_hours_minutes>\d*)\s*,
EVAL-uptime_fromdays = (uptime_days*86400)
EVAL-uptime_fromminutes = (uptime_minutes*60)
EVAL-uptime_fromhours = (uptime_hours_hours*60*60) + (uptime_hours_minutes*60)
EVAL-uptime = case(isnotnull(uptime_daysmin), ((uptime_days*86400)+(uptime_daysmin*60)), isnotnull(uptime_days), ((uptime_days*86400)+(uptime_dayshour*3600)+(uptime_daystime*60)), isnotnull(uptime_minutes), (uptime_minutes*60), isnotnull(uptime_hours), (uptime_hours_hours*60*60) + (uptime_hours_minutes*60) )

# CIM Network
REPORT-inventory_network_extractions = inventory_network_interface, inventory_network_ip, inventory_network_mac

###########################################
#			nmon clean stanza
###########################################

[nmon_clean]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_PREFIX=^
TIME_FORMAT=%d-%m-%Y %H:%M:%S
MAX_TIMESTAMP_LOOKAHEAD=19
LINE_BREAKER=([\n\r]+)\d{2}-\d{2}-\d{4}\s\d{2}:\d{2}:\d{2}
TRUNCATE=999999

# Deactivate KV
KV_MODE = none

###########################################
#			nmon collect stanza
###########################################

[nmon_collect]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_PREFIX=^
TIME_FORMAT=%d-%m-%Y %H:%M:%S
MAX_TIMESTAMP_LOOKAHEAD=19
LINE_BREAKER=([\n\r]+)\d{2}-\d{2}-\d{4}\s\d{2}:\d{2}:\d{2}
TRUNCATE=999999

# Deactivate KV
KV_MODE = none

##############################################
#			SYSLOG SPECIAL SECTIONS					#
##############################################

# These parameters are dedicated to the deployment of Nmon using syslog as the transport layer
# to forward Nmon Performance data from your end servers to your central syslog, and finally to Splunk

# Deploying Nmon with syslog requires additional configuration on search heads, and potentially indexers
# This also requires specific configuration on end clients (rsyslog/syslog-ng config, cron config, logrotate)

# Data being transferred through syslog is not csv structured data but key=value data
# each source of data is being in a temporary sourcetype for indexing time parsing to occur, then we will
# rewrite the sourcetypes to match standard sourcetypes being used by Core application

# depending on its origin (the source, normal vs syslog), the configuration on the sh head will apply the correct
# extraction parameters at search time.

# The nmon-logger package for Syslog deployment is available at:
# https://github.com/guilhemmarchand/nmon-logger

# Online guides for Syslog deployment:
# rsyslog: http://nmonsplunk.wikidot.com/documentation:installation:rsyslog
# syslog-ng: http://nmonsplunk.wikidot.com/documentation:installation:syslog-ng

####################
# Syslog to Splunk #
####################
#
# If you receive directly from syslog over TCP/UDP, use this sourcetype
# Create a dedicated UDP or TCP input with:
# index = nmon
# source = (leave optional or put anything you like)
# sourcetype = nmon:fromsyslog

# This sourcetype must manage incoming multi / or mono line event
# syslog timestamp is not being used for data time stamping (using nmon timestamp)

#
# Splunk to Splunk is not recommended, prefer having a Splunk instance installed on rsyslog collectors
# or on machine receiving rsyslog data, and generating local per host files
# Therefore, it is still possible to achieve it using a tcp/udp input using this sourcetype:

[nmon:fromsyslog]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
LINE_BREAKER=([\r\n]+)[a-zA-Z0-9\-\_\:\s]*timestamp=\"
MAX_EVENTS=100000
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
TRUNCATE=0
MAX_TIMESTAMP_LOOKAHEAD=26

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS-syslog = syslog-host

# Additional: In full extracted mode, we want 2 basic Nmon extracted at indexed time
TRANSFORMS-nmon_data_kv_createindexed_time = nmon_data_kv_createindexed_OStype, nmon_data_kv_createindexed_type

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-perfdata_fromsyslog = nmon_data_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_config
TRANSFORMS-configdata_fromsyslog = nmon_config_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_processing
TRANSFORMS-processingdata_fromsyslog = nmon_processing_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_collect
TRANSFORMS-collectdata_fromsyslog = nmon_collect_fromsyslog_rewrite

# Rewrite sourcetype to standard nmon_clean
TRANSFORMS-cleandata_fromsyslog = nmon_clean_fromsyslog_rewrite

#####################################
# Splunk to Splunk with syslog data #
#####################################

# These sourcetypes will be used when reading local files generated by syslog from remote syslog clients
# Each type of data must have its own file monitor.

# See inputs.conf.spec for more information, or read the online wiki manual pages for rsyslog / syslog-ng deployments

#
# For nmon performance data:
#

###################################################
# Set parameters for nmon_data coming from syslog #
###################################################

[nmon_data:fromsyslog]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
MAX_TIMESTAMP_LOOKAHEAD=26
KV_MODE=auto

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS-syslog = syslog-host

# Additional: In full extracted mode, we want 2 basic Nmon extracted at indexed time
TRANSFORMS-nmon_data_kv_createindexed_time = nmon_data_kv_createindexed_OStype, nmon_data_kv_createindexed_type

# Rewrite sourcetype to standard nmon_data
TRANSFORMS-perfdata_fromsyslog = nmon_data_fromsyslog_rewrite

# For search heads, activate kvmode to auto for that source
[source::perfdata:syslog]
KV_MODE=auto

#
# For nmon configuration data:
#

#####################################################
# Set parameters for nmon_config coming from syslog #
#####################################################

[nmon_config:fromsyslog]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
LINE_BREAKER=([\r\n]+)[a-zA-Z0-9\-\_\:\s]*timestamp=\"
MAX_EVENTS=100000
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
TRUNCATE=0

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS-syslog = syslog-host

# Rewrite sourcetype to standard nmon_config
TRANSFORMS-configdata_fromsyslog = nmon_config_fromsyslog_rewrite

#
# For nmon processing data:
#

[nmon_processing:fromsyslog]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
TIME_FORMAT=%d-%m-%Y %H:%M:%S
TIME_PREFIX=nmon2csv:
CHARSET=UTF-8
BREAK_ONLY_BEFORE=nmon2csv:

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS-syslog = syslog-host

# Rewrite sourcetype to standard nmon_processing
TRANSFORMS-processingdata_fromsyslog = nmon_processing_fromsyslog_rewrite

#
# For nmon collecting data:
#

[nmon_collect:fromsyslog]

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS-syslog = syslog-host

# Rewrite sourcetype to standard nmon_collect
TRANSFORMS-collectdata_fromsyslog = nmon_collect_fromsyslog_rewrite

#
# For nmon clean data:
#

[nmon_clean:fromsyslog]
SHOULD_LINEMERGE=true
NO_BINARY_CHECK=true
BREAK_ONLY_BEFORE=Starting nmon cleaning
TIME_FORMAT=%c
TIME_PREFIX=\w*\s
CHARSET=UTF-8

# Manage carriage returns sent by syslog
SEDCMD-carriage_return2 = s/#012/\n/g

# Rewrite host Metadata using standard syslog rewrite
TRANSFORMS-syslog = syslog-host

# Rewrite sourcetype to standard nmon_clean
TRANSFORMS-cleandata_fromsyslog = nmon_clean_fromsyslog_rewrite

##############################################
#			SPLUNK HEC (HTTP INPUT)
##############################################

# It is possible to forward nmon data over HTTP using the TA-nmon-hec and nmon-logger-hec packages

# TA-nmon-hec: the package is availanle at: https://splunkbase.splunk.com/app/3668
# nmon-logger: the package is available at: https://github.com/guilhemmarchand/nmon-logger

# In this scenario, the data and is streamed directly to the HEC input

### nmon_data ###

[nmon_data:fromhttp]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
MAX_TIMESTAMP_LOOKAHEAD=26
KV_MODE=auto

# Additional: In full extracted mode, we want 2 basic Nmon extracted at indexed time
TRANSFORMS-nmon_data_kv_createindexed_time = nmon_data_kv_createindexed_OStype, nmon_data_kv_createindexed_type

# Rewrite the source Metadata to manage search time extraction
TRANSFORMS-perfdata_rewrite_meta = nmon_data_fromhttp_rewrite_host, nmon_data_fromhttp_rewrite_source, nmon_data_fromhttp_rewrite_sourcetype

# For search heads, activate kvmode to auto for that source
[source::perfdata:http]
KV_MODE=auto

### nmon_config ###

[nmon_config:fromhttp]
SHOULD_LINEMERGE=false
NO_BINARY_CHECK=true
CHARSET=UTF-8
LINE_BREAKER=([\r\n]+)timestamp=\"
MAX_EVENTS=100000
TIME_FORMAT=%s
TIME_PREFIX=timestamp="
TRUNCATE=0

# Rewrite the source Metadata to manage search time extraction
TRANSFORMS-configdata_rewrite_meta = nmon_config_fromhttp_rewrite_host, nmon_config_fromhttp_rewrite_source, nmon_config_fromhttp_rewrite_sourcetype

# For search heads
[source::configdata:http]
KV_MODE=none
